{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as plb\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "from utils import CLRS\n",
    "import utils\n",
    "\n",
    "params = {\n",
    "    'axes.titlesize': 16, \n",
    "    'axes.labelsize': 14,\n",
    "    'legend.fontsize': 12,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12}\n",
    "\n",
    "plb.rcParams.update(params)\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette(\"muted\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "Machine Predictive Maintenance Classification Dataset <br>\n",
    "Since real predictive maintenance datasets are generally difficult to obtain and in particular difficult to publish, <br>\n",
    "we present and provide a synthetic dataset that reflects real predictive maintenance encountered in the industry to the best of our knowledge.\n",
    "\n",
    "The dataset consists of 10 000 data points\n",
    "\n",
    "| Column | Description |\n",
    "| --- | --- |\n",
    "UID | unique identifier ranging from 1 to 10000 |\n",
    "Product ID | consisting of a letter L, M, or H for low (50% of all products), medium (30%), and high (20%) as product quality variants and a variant-specific serial number |\n",
    "Air temperature [K] | generated using a random walk process later normalized to a standard deviation of 2 K around 300 K |\n",
    "Process temperature [K] | generated using a random walk process normalized to a standard deviation of 1 K, added to the air temperature plus 10 K.\n",
    "Rotational speed [rpm] | calculated from powepower of 2860 W, overlaid with a normally distributed noise\n",
    "Torque [Nm] | torque values are normally distributed around 40 Nm with an Ïƒ = 10 Nm and no negative values.\n",
    "Tool wear [min] | The quality variants H/M/L add 5/3/2 minutes of tool wear to the used tool in the process. and a\n",
    "Target | label that indicates, whether the machine has failed in this particular data point for any of the following failure modes are true.\n",
    "\n",
    "\n",
    "\n",
    "Important : There are two Targets - Do not make the mistake of using one of them as feature, as it will lead to leakage.\n",
    "- Target : Failure or Not\n",
    "- Failure Type : Type of Failure\n",
    "\n",
    "Acknowledgements\n",
    "UCI : https://archive.ics.uci.edu/ml/datasets/AI4I+2020+Predictive+Maintenance+Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('machine_predictive_maintenance.csv', index_col='UDI')\n",
    "print(df.shape)\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Failure Type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Preprocessing\n",
    "\n",
    "1. Check and fix data types and remove duplicates\n",
    "2. Check and fix missing and unlikely values\n",
    "3. Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE UNITS\n",
    "# TODO: XGBoost does not allow brackets, just replace with parenthesis\n",
    "df.columns = [re.sub(r'\\[.+?\\]\\s*', '', x).strip() for x in df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check and fix data types and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK DATA TYPES\n",
    "display(df.dtypes)\n",
    "\n",
    "# CHECK DUPLICATES\n",
    "print('No. of duplicated records across all columns: ', df.duplicated().sum())\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for and fix missing and unlikely values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK FOR MISSING VALUES\n",
    "def get_percent_null(df):\n",
    "    x = df.isnull().sum()*100 / len(df)\n",
    "    return display(x[x>0].to_frame('precent_nulls'))\n",
    "\n",
    "get_percent_null(df)\n",
    "\n",
    "na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', \n",
    "    '-NaN', '-nan', '1.#IND', '1.#QNAN', '<NA>', 'N/A', 'NA', 'NULL', \n",
    "    'NaN', 'n/a', 'nan', 'null', 'None', 'none', 'na']\n",
    "\n",
    "print('Check if there are other types of missing values:')\n",
    "for x in na_values:\n",
    "    df_ = (df == x).sum()\n",
    "    if df_.sum() > 0:\n",
    "        print(f'The ff column(s): {df_[df_ > 0].index.to_list()} contain(s) ', f'\"{x}\"') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK FOR UNLIKELY VALUES\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity checks \n",
    "\n",
    "- Check outliers from expected acceptable measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Misc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP NON-USEFUL COLUMNS\n",
    "df.drop(['Product ID', 'Failure Type'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Class imbalanced; choose proper eval metric(s), oversample minority class (?), ~10% min class\n",
    "df.Target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate/group columns\n",
    "c = df.dtypes == object\n",
    "\n",
    "TARGET_COL = 'Target'\n",
    "NUM_COLS   = list(df.select_dtypes(include=np.number).columns)\n",
    "CAT_COLS   = list(c[c].index)\n",
    "\n",
    "NUM_COLS.remove(TARGET_COL)\n",
    "\n",
    "DROP_COLS_CANDIDATES = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationships Among Numeric Features and Target Variable\n",
    "\n",
    "### Correlation Among Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,5))\n",
    "df_ = df[NUM_COLS].corr(method='spearman').round(2)\n",
    "ax = sns.heatmap(df_, annot=True, vmin=-1, vmax=1, cbar=True, cmap='Blues')\n",
    "\n",
    "df_ = utils.get_trunc_corr_df(df_, 0.7)\n",
    "df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Air temperature and Process temparature are highly correlated. \n",
    "- Rotational speed and Torque are highly correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_COLS_CANDIDATES.update(['Air temperature', 'Rotational speed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure Strength of Association of each Feature with Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add correlation rank biserial correlation, chi square etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 5, figsize=[16,6.5])\n",
    "\n",
    "for i, c in enumerate(NUM_COLS):\n",
    "    sns.histplot(data=df, x=c, kde=True, ax=axes[0,i], hue='Target', palette=[CLRS['blue'],CLRS['red']])\n",
    "    sns.boxplot(data=df, y=c, x='Target', ax=axes[1,i], palette=[CLRS['lblue'],CLRS['lred']])\n",
    "    print('{} skewness = {}'.format(c, round(df[c].skew(), 2)))\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "# TODO: How to interpret skewness and kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It seems that failure occurs at higher value except for rotational speed which is just inversely proportional to torque\n",
    "- There are visible differences in the distribution of features in each target class which is good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check which pairs of features separate the data well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(16,6.5))\n",
    "axs = axs.flatten()\n",
    "for i, c in enumerate(combinations(NUM_COLS, 2)):\n",
    "    sns.scatterplot(data=df, x=c[0], y=c[1], ax=axs[i], hue = 'Target', palette=[CLRS['lblue'],CLRS['red']])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationships Among Categorical Features and Target Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure Strength of Association of each Feature with Target Variable\n",
    "\n",
    "- Cramer's V; Rule of thumb for strength of association -- Deg. of freedom = 1; > 0.1 - Weak, > 0.3 - Medium, > 0.5 - Strong\n",
    "- NOTE: V has different threshold depending on usecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_corr = []\n",
    "for c in CAT_COLS:\n",
    "    cv_corr.append(utils.cramers_corrected_stat(pd.crosstab(df[c], df[TARGET_COL])))\n",
    "fig = plt.figure(figsize=(1.5*len(CAT_COLS),1))\n",
    "ax = sns.heatmap(\n",
    "    pd.DataFrame(cv_corr, index=CAT_COLS, columns=[TARGET_COL]).sort_values(by=TARGET_COL).T, \n",
    "    vmin=0, vmax=1, annot=True, cbar=True, cmap='Blues')\n",
    "x = ax.set_xticklabels(ax.get_xticklabels(), rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.groupby('Type')['Target'].size().to_frame('num_total')\n",
    "y = df.groupby('Type')['Target'].sum().to_frame('num_failures')\n",
    "z = pd.DataFrame(100 * (y.values/x.values).round(4), columns=['pct_failures'], index=x.index)\n",
    "x.join(y).join(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As expected, there are higher percentage of failure for lower quality product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways, Recommendations and Next Steps for EDA\n",
    "- No major cleaning for the data since it is synthetic and properly generated\n",
    "- The features in the dataset seems to be "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import (precision_score, recall_score, \n",
    "    f1_score, log_loss, ConfusionMatrixDisplay,\n",
    "    classification_report, roc_curve, auc) # --- not used\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import catboost as CB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import shap\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "1. Handle Outliers\n",
    "2. Encode/Transform Data\n",
    "3. Drop Highly Correlated Features\n",
    "4. Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDLE OUTLIERS\n",
    "OUTLR_COLS = ['Rotational speed', 'Torque']\n",
    "boxcox_tx = PowerTransformer(method='box-cox', standardize=False)\n",
    "df[OUTLR_COLS] = boxcox_tx.fit_transform(df[OUTLR_COLS])\n",
    "\n",
    "# ENCODE/TRANFORM DATA\n",
    "df['Type'].replace({'L':0, 'M':1, 'H':2}, inplace=True)\n",
    "\n",
    "# DROP HIGHLY CORRELATED FEATURES\n",
    "#\n",
    "\n",
    "# Z-SCORE NORMALIZE DATA\n",
    "df[NUM_COLS] = StandardScaler().fit_transform(df[NUM_COLS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Baseline Model\n",
    "\n",
    "1. Feature combinations to try: [NUM_COLS + CAT_COLS], [NUM_COLS - DROP_COLS_CANDIDATES + CAT_COLS]\n",
    "2. ML Classifiers to try: LogisticRegression, CATBoost, XGBoost, RandomForest, SVM\n",
    "3. Evalutaion metrics to try: Precision, Recall, F1, Log Loss, PR-AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Train Set and Test Set\n",
    "- 80-20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = \\\n",
    "    train_test_split(\n",
    "        df[NUM_COLS+CAT_COLS], df[TARGET_COL], test_size = 0.2, random_state = 100\n",
    "                )\n",
    "\n",
    "sum(y_test)/len(y_test), sum(y_train)/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OVER-SAMPLE AND UNDER-SAMPLE THE TRAIN SET\n",
    "# According to the user guide, ENN tends to clean more noisy samples than Tomek\n",
    "# smote_enn = SMOTEENN(random_state=42) \n",
    "\n",
    "# x_train, y_train = smote_enn.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add LightGBM Classifier\n",
    "\n",
    "LR_clf  = LogisticRegression(random_state = 100)\n",
    "CB_clf  = CB.CatBoostClassifier(random_state = 100, verbose=False)\n",
    "XGB_clf = XGBClassifier(random_state = 100)\n",
    "RF_clf  = RandomForestClassifier(random_state = 100)\n",
    "SVM_clf = SVC(random_state = 100)\n",
    "\n",
    "classifiers  = {'LR_clf': LR_clf, 'CB_clf': CB_clf, 'XGB_clf': XGB_clf, 'RF_clf': RF_clf, 'SVM_clf': SVM_clf}\n",
    "eval_metrics = {'precision': precision_score, 'recall': recall_score, 'f1': f1_score, \n",
    "                'neg_log_loss': log_loss, 'pr_auc': utils.pr_auc_score}\n",
    "feature_sets = {'feat_set1': NUM_COLS + CAT_COLS, \n",
    "                'feat_set2': list(set(NUM_COLS) - DROP_COLS_CANDIDATES) + CAT_COLS}\n",
    "\n",
    "print(len(feature_sets['feat_set1']), len(feature_sets['feat_set2']), sep=' | ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_performance = {}\n",
    "\n",
    "for featr_k, featr_v in feature_sets.items():\n",
    "    x_train_ = x_train[featr_v]\n",
    "    x_test_ = x_test[featr_v]\n",
    "\n",
    "    clfs = {}\n",
    "    for clf_k, clf_v in classifiers.items():\n",
    "        clf_v = clf_v.fit(x_train_, y_train)\n",
    "        y_pred = clf_v.predict(x_test_)\n",
    "\n",
    "        eval_scores = {}\n",
    "        for metric_k, metric_v in eval_metrics.items():\n",
    "            eval_scores.update({metric_k:metric_v(y_test, y_pred)})\n",
    "        \n",
    "        clfs.update({clf_k:eval_scores})\n",
    "\n",
    "    clf_performance.update({featr_k:clfs})\n",
    "\n",
    "del x_train_, x_test_, clfs, eval_scores  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Models and Choose Final Model(s)\n",
    "\n",
    "TODO: \n",
    "- Should baseline model be chosen based on training performance or test or both? Are there metrics to consider both?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_df = pd.DataFrame.from_dict(\n",
    "    {(i,j): clf_performance[i][j] \n",
    "        for i in clf_performance.keys() \n",
    "        for j in clf_performance[i].keys()},\n",
    "    orient='columns').round(3)\n",
    "perf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The best performing base model is XGBoost (4/5 metrics) although CatBoost and RandomForest are close. \n",
    "    - We'll do hyperparameter tuning for the 3\n",
    "    - [Tree-based models generally perform well for imbalanced dataset.](https://stackoverflow.com/questions/46104173/why-decision-tree-works-perfect-on-imbalanced-data#:~:text=For%20simple%20classifiers%20using%20linear%20regression%2C%20such%20as,the%20model%20sorts%20all%20samples%20into%20most%20label.)\n",
    "- LogisticRegression generally performed worst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model\n",
    "\n",
    "- Feature selection \n",
    "    - recursive feature elimination (RFE)\n",
    "    - TODO: Try other [methods](https://scikit-learn.org/stable/modules/feature_selection.html)\n",
    "- GridSeachCV (hyperparameter tuning + cross validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = 'feat_set2'\n",
    "final_model = XGB_clf\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=final_model,\n",
    "    step=1,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    min_features_to_select=1,\n",
    ")\n",
    "rfecv.fit(x_train, y_train)\n",
    "\n",
    "print(rfecv.n_features_)\n",
    "\n",
    "# Columns to drop\n",
    "drop_cols = [feature_sets[feature_set][i] for i,j in enumerate(rfecv.ranking_ != 1) if j]\n",
    "print(drop_cols)\n",
    "\n",
    "if not drop_cols:\n",
    "    x_train.drop(drop_cols, axis=1, inplace=True)\n",
    "    x_test.drop(drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Optimal number of features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV\n",
    "\n",
    "CB_param_grid = {\n",
    "    'n_estimators': [100, 200, 400],\n",
    "    'learning_rate': [0.05, 0.10, 0.15],\n",
    "    'max_depth': [4, 6, 8]}\n",
    "\n",
    "XGB_param_grid = {\n",
    "    'learning_rate': [0.05, 0.10, 0.15],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'booster':['gblinear', 'gbtree', 'dart']}\n",
    "\n",
    "RF_param_grid = { \n",
    "    'n_estimators': [100, 200, 400],\n",
    "    'max_depth' : [4, 6, 8],\n",
    "    'criterion' :['gini', 'entropy']}\n",
    "\n",
    "param_grids = {'CB_clf':CB_param_grid, 'XGB_clf':XGB_param_grid, 'RF_clf':RF_param_grid}\n",
    "base_models = ['CB_clf', 'XGB_clf', 'RF_clf']\n",
    "\n",
    "# TODO: Try multiple metrics during grid search\n",
    "def gscv_helper(X, y, model, param_grid, **kwargs):\n",
    "    gscv = GridSearchCV(\n",
    "        estimator=model, \n",
    "        param_grid=param_grid,\n",
    "        scoring=list(eval_metrics.keys())[:-1],\n",
    "        refit='f1',\n",
    "        return_train_score=True)\n",
    "    return gscv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_gscv_df = {}\n",
    "modl_gscv_df = {}\n",
    "for k in base_models:\n",
    "    m = gscv_helper(x_train, y_train, classifiers[k], param_grids[k])\n",
    "    y = m.predict(x_test)\n",
    "\n",
    "    eval_scores = {}\n",
    "    for metric_k, metric_v in eval_metrics.items():\n",
    "        eval_scores.update({metric_k:round(metric_v(y_test, y),3)})\n",
    "\n",
    "    perf_gscv_df.update({k:eval_scores})\n",
    "    modl_gscv_df.update({k:[m,y]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned nmodels performance\n",
    "tuned_perf_df = pd.DataFrame.from_dict(\n",
    "    {(i): perf_gscv_df[i] for i in perf_gscv_df.keys()}, \n",
    "    orient='index').T\n",
    "tuned_perf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base models performance\n",
    "perf_df['feat_set1'][base_models]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CatBoost had better improvement\n",
    "- TODO: Tune XGBoost (and the rest) even more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate/ Validate Model/ Check Scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_perf_df.XGB_clf.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot train vs test performance\n",
    "\n",
    "- no overfitting\n",
    "- TODO: Do more in terms of checking if model overfitted e.g. check bias and variance of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv_res = modl_gscv_df['XGB_clf'][0]\n",
    "final_model = gscv_res.best_estimator_\n",
    "y_pred = modl_gscv_df['XGB_clf'][1]\n",
    "gscv_res.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = gscv_res.cv_results_\n",
    "\n",
    "dfs = []\n",
    "scoring = list(eval_metrics.keys())[:-1]\n",
    "for s in scoring:\n",
    "    t1 = f'mean_train_{s}'\n",
    "    t2 = f'mean_test_{s}'\n",
    "    t3 = ['train','test']\n",
    "    df_ = pd.DataFrame([res[t1], res[t2]], index=t3).T * 100\n",
    "    df_ = pd.melt(df_, var_name='data', value_name=s, value_vars=t3, ignore_index=False)\n",
    "    dfs.append(df_.set_index('data', append=True))\n",
    "\n",
    "df_ = pd.concat(dfs, axis=1).reset_index().rename(columns={'level_0':'id'}) \n",
    "\n",
    "fig, axs = plt.subplots(1, len(scoring), figsize=(13,3.25))\n",
    "for i,s in enumerate(scoring):\n",
    "    sns.lineplot(\n",
    "        x='id', y=s, hue='data', data=df_, ax=axs[i], \n",
    "        palette=[CLRS['lblue'],CLRS['red']])\n",
    "    if i != len(scoring)-1:\n",
    "        axs[i].set_ylim(0,105)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC_AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make function to draw the plot ROC to easly\n",
    "def plot_roc_(fpr,tpr,roc_auc):\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, color=CLRS['red'], label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],linestyle='--', color=CLRS['lblue'])\n",
    "    plt.axis('tight')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba_log=final_model.predict_proba(x_test)\n",
    "fpr_b, tpr_b, thresholds_b = roc_curve(y_test,y_proba_log[:,1])\n",
    "roc_auc_b = auc(fpr_b, tpr_b)\n",
    "plot_roc_(fpr_b,tpr_b,roc_auc_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot confusion matrix\n",
    "\n",
    "- 0.28% false positive, 0.0021% false negative\n",
    "- TODO: review other metrics such as sensitivity and specificity, when to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, y_pred, colorbar=False, cmap='Blues')#, normalize='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "NOTE: \n",
    "1. XGBoost has no internal feature importance for 'gbtree' and 'dart' boosters\n",
    "2. Other options for tree-based models:\n",
    "    - Mean decrease in impurity (MDI)\n",
    "    - Feature permutation\n",
    "    - [SHAP](https://shap.readthedocs.io/en/latest/index.html)\n",
    "3. Other option is to retrain model outside of grid search using best params\n",
    "\n",
    "\n",
    "TODO: \n",
    "- Do more feature importance, e.g. check p-value of the features (?), Handle featuer importance properly when there are [highly correlated features](https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-multicollinear-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Built-in Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost has no built-in feature importance for 'gbtree' booster\n",
    "# final_model.feature_importances_.argsort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP-based\n",
    "\n",
    "- TODO: Study SHAP, how it works and how to interpret results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(final_model, random_state=100)\n",
    "shap_values = explainer.shap_values(x_train)\n",
    "shap.summary_plot(shap_values, x_train, plot_type=\"bar\", color=CLRS['lblue'])\n",
    "shap.summary_plot(shap_values, x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permutation-based\n",
    "\n",
    "- TODO: Study how this works and how to interpret results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = permutation_importance(final_model, x_test, y_test, random_state=100)\n",
    "pi = pd.DataFrame(\n",
    "    [x_train.columns, pi['importances_mean']], \n",
    "    index=['Features', 'Importances (mean)']) \\\n",
    "        .T.sort_values('Importances (mean)', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(y=pi['Features'], x=pi['Importances (mean)'], orient='h', color=CLRS['lblue'])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Scalability\n",
    "\n",
    "- Scalability of the models in terms of computational cost e.g. training and scoring times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv_res.cv_results_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time']\n",
    "for i in l:\n",
    "    print(i, round(gscv_res.cv_results_[i][gscv_res.best_index_], 5), sep=':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('XGB_final_model.pkl', 'wb') as f:\n",
    "    pickle.dump(final_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Takeaways, Recommendations and Next Steps\n",
    "\n",
    "- Trying different models, we found out that XGBoost performs best <br>\n",
    "with the following metric scores during final modelling: \n",
    "\n",
    "        ```\n",
    "        {'precision': 0.926,\n",
    "        'recall': 0.725,\n",
    "        'f1': 0.813,\n",
    "        'neg_log_loss': 0.397,\n",
    "        'pr_auc': 0.83}\n",
    "        ```\n",
    "Next Steps:\n",
    "- Benchmark model\n",
    "- Adjust probability threshold to let the business decide what works best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "1. Choose better plot colors e.g. PoerBI defaults\n",
    "2. Do all TODOs and put notes for those that are new to me, e,g, how roc_auc works and how to interpret\n",
    "\n",
    "### NOTES\n",
    "- Adding duymmyfied failure types as feattures: This is highly associated with the target variable, <br>\n",
    "dummyfying it would still result in leakage since the target variable would be like a linear combination of the resulting columns. \n",
    "- On oversampling the minority class: Just choose proper methods instead of creating synthetic data that could change the actual distribution of data.\n",
    "- Do not touch test set other than for testing e.g. if upsampling, use only train set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
